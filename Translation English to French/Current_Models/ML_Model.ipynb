{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiC75uo6u_Of"
   },
   "source": [
    "##Machine Translation Using a Seq2Seq Architecture\n",
    "\n",
    "---\n",
    "The goal of this colab is to get you more familiar with the Seq2Seq models and their challenges. For this reason, you will be working on machine translation problem where we would have a sentence as input (in english), and the output is gonna be the translated sentence (in french). So just like what happens with Google Translate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeK4LPupvg_c"
   },
   "source": [
    "**Just to give you a heads up:** We won't be having a model performing like Google translate, but at least we will have an idea about how Google Translate works and the challenges that exist with a translation problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBTvDTzBv293"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_j1ZzS3v6N3"
   },
   "source": [
    "We start by importing numpy and pandas and then we can add the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0IARXAX1e1m"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAcLqZ7uv-SJ"
   },
   "source": [
    "Upload your data here. Here is the [Drive link](https://drive.google.com/drive/folders/10ncj3w7kI9GPx_rz-WfKEGCv4Dz1EYf6?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3hLN42axOjn"
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-M7cFxTPpqy"
   },
   "outputs": [],
   "source": [
    "en = pd.read_csv('Datasets/en.csv')\n",
    "fr = pd.read_csv('Datasets/fr.csv')\n",
    "english_sentences = en.iloc[:, 0]\n",
    "french_sentences = fr.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr8OO1OhwSp4"
   },
   "source": [
    "**How many sentences does each of the files contain?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhWJP-b02HKq"
   },
   "outputs": [],
   "source": [
    "len(english_sentences), len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITGJN5tIwkDO"
   },
   "source": [
    "Now let us concatenate the 2 dataframes into one dataframe that we call **df** where one column has the english senetnces and the other has the french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZXxahsB2njn"
   },
   "outputs": [],
   "source": [
    "en.columns = ['English']\n",
    "fr.columns = ['French']\n",
    "combined_df = pd.concat([en, fr], axis=1)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc1TsEHw9yC"
   },
   "source": [
    "Pick a sentence and print it in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuRVWch23ujo"
   },
   "outputs": [],
   "source": [
    "print(combined_df.iloc[0, 0])\n",
    "print(combined_df.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQjXYP1txFCi"
   },
   "source": [
    "##Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgz6jIoVxHUF"
   },
   "source": [
    "The data that we have is almost clean as we can see, we just need to remove the punctuations inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YYOt5QftcFI"
   },
   "outputs": [],
   "source": [
    "'''remove the punctuation'''\n",
    "combined_df['English'] = combined_df['English'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "combined_df['French'] = combined_df['French'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "''' convert to lowercase'''\n",
    "combined_df['English'] = combined_df['English'].apply(lambda x: x.lower())\n",
    "combined_df['French'] = combined_df['French'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C1qsC9LxZPb"
   },
   "source": [
    "Make sure that the punctuation is removed by printing the example that you printed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T80tiWxe84G7"
   },
   "outputs": [],
   "source": [
    "combined_df.iloc[0, 0], combined_df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuFNjoBAx4oN"
   },
   "source": [
    "##Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATfefzPExi2k"
   },
   "source": [
    "Add a column **ENG Length** to the dataset that shows how many words does a sentence contain, and do the same for french in a column called **FR Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dakeo81s352S"
   },
   "outputs": [],
   "source": [
    "# add a column to the dataframe called \"ENG Length\" and \"FR Length\" that contains the length of the English and French sentences respectively\n",
    "combined_df['ENG Length'] = combined_df['English'].apply(lambda x: len(x.split()))\n",
    "combined_df['FR Length'] = combined_df['French'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjQLW0K5xwx1"
   },
   "source": [
    "Visualize the distribution of the lengths of english sentences and french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q_UIMJ09L24"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(combined_df['ENG Length'], bins=10, alpha=0.5, label='English Sentence Lengths',density=True) # density=True normalizes the data\n",
    "plt.hist(combined_df['FR Length'], bins=10, alpha=0.5, label='French Sentence Lengths', density=True) # density=True normalizes the data (getting an approximation of the pdf)\n",
    "plt.xlabel('Sentence Length (Word Count)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths (English vs. French)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDXb2d9ix9DV"
   },
   "source": [
    "Get the maximum length of an english sentence and the maximum length of a french sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpnBB04U_lHd"
   },
   "outputs": [],
   "source": [
    "max_eng_length = combined_df['ENG Length'].max()\n",
    "max_fr_length = combined_df['FR Length'].max()\n",
    "max_eng_length, max_fr_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4s-spsRyGJv"
   },
   "source": [
    "##Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0ZmIT2GyJMU"
   },
   "source": [
    "In order for the data to be fed to the model, it has to be tokenized and padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0r9z-eErm9H"
   },
   "source": [
    "####Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5L_zkhfyQuX"
   },
   "source": [
    "**To tokenize english and french sentences, we can use only one tokenizer. True or False?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z0ZcNOeyauD"
   },
   "source": [
    "Wrong, in this case I am using basic word tokenization through nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "814mKDFiymcY"
   },
   "source": [
    "Tokenize the sentences that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiXlciqFuQzW"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "english_sentences = combined_df['English']\n",
    "french_sentences = combined_df['French']\n",
    "english_tokenized = tokenize_sentences(english_sentences)\n",
    "french_tokenized = tokenize_sentences(french_sentences)\n",
    "print(english_tokenized[0])\n",
    "print(french_tokenized[0])\n",
    "print(len(english_tokenized), len(french_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUN01jDXys9B"
   },
   "source": [
    "**How many unique words do we have in english and in french?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WahkdzKvIlO"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_sentences, special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]):\n",
    "    all_tokens_list = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        all_tokens_list.extend(sentence)\n",
    "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    for token in all_tokens_list:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab) # updating each token with a unique index, since len(vocab) is changing each time\n",
    "    # Decoding Later \n",
    "    index_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    return vocab, index_to_token\n",
    "english_vocab, english_index_to_token = build_vocabulary(english_tokenized)\n",
    "french_vocab, french_index_to_token = build_vocabulary(french_tokenized)\n",
    "\n",
    "# save these dictionaries for later use\n",
    "import pickle\n",
    "with open('english_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(english_vocab, f)\n",
    "with open('english_index_to_token.pkl', 'wb') as f:\n",
    "    pickle.dump(english_index_to_token, f)\n",
    "with open('french_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(french_vocab, f)\n",
    "with open('french_index_to_token.pkl', 'wb') as f:\n",
    "    pickle.dump(french_index_to_token, f)\n",
    "\n",
    "print(\"English Vocabulary Size:\", len(english_vocab))\n",
    "print(\"French Vocabulary Size:\", len(french_vocab))\n",
    "print(english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0C2RJjArtJd"
   },
   "source": [
    "#### Padding + Batching + putting them in sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXdTXMo5y8oB"
   },
   "source": [
    "**What should be the length of the sequences that we have after padding?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wtHQsgXzImq"
   },
   "source": [
    "As the padding length, in this case I will take as the length of the maximum phrase here!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRXayRzVzQD4"
   },
   "source": [
    "Perform padding on the sequences that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNdO9EZrxvmN"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_src_len, max_tgt_len):\n",
    "        '''\n",
    "        src_sentences: List of source sentences (as tokens) -> English\n",
    "        tgt_sentences: List of target sentences (as tokens) -> French\n",
    "        src_vocab: Source vocabulary -> Maps English Tokens to indices\n",
    "        tgt_vocab: Target vocabulary -> Maps French Tokens to indices\n",
    "        max_src_len: Maximum length of source sentences\n",
    "        max_tgt_len: Maximum length of target sentences\n",
    "        '''\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        # determine the dataset length, for later use !!!!\n",
    "        return len(self.src_sentences) \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx: Index of the sentence pair to retrieve\n",
    "\n",
    "        Returns:\n",
    "        src_indices: Indices of the source sentence\n",
    "        src_length: Actual length of the source sentence\n",
    "        tgt_indices: Indices of the target sentence\n",
    "        '''\n",
    "        # Get english(source) and target sentence at this index \n",
    "        src_sentence = self.src_sentences[idx]\n",
    "        tgt_sentence = self.tgt_sentences[idx]\n",
    "        \n",
    "        # Convert this sentence to indices (of course for each token), and pad them to the max length(liek talked about in the report)\n",
    "        src_indices = self._convert_and_pad(src_sentence, self.src_vocab, self.max_src_len, \"<PAD>\",adding_sos_eos=False)\n",
    "        tgt_indices = self._convert_and_pad(tgt_sentence, self.tgt_vocab, self.max_tgt_len, \"<PAD>\",adding_sos_eos=True)\n",
    "        # Get actual lengths (ignoring <PAD> tokens) (like we talked about in the report)\n",
    "        src_length = min(len(src_sentence), self.max_src_len) # actual length of the source sentence, which we will need in the encoder\n",
    "        # tgt_length = min(len(tgt_sentence), self.max_tgt_len), we don't need this, because we are not using it in the decoder\n",
    "        return torch.tensor(src_indices), src_length, torch.tensor(tgt_indices)\n",
    "    def _convert_and_pad(self, sentence, vocab, max_len, pad_token,adding_sos_eos=False):\n",
    "        '''Convert tokens to indices'''\n",
    "        if adding_sos_eos:\n",
    "            sentence = [\"<SOS>\"] + sentence + [\"<EOS>\"]\n",
    "        indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in sentence] # maps each token to its index in the vocabulary, if not found, it maps it to the index of the <UNK> token\n",
    "        '''Truncate or pad to max length'''\n",
    "        indices = indices[:max_len] + [vocab[pad_token]] * (max_len - len(indices))\n",
    "        return indices\n",
    "def collate_batch(batch):\n",
    "    '''\n",
    "    This function is used to combine individual samples into batches with consistent shapes, required by the DataLoader\n",
    "\n",
    "    batch: List of individual sample, where each sample is a tuple \"returnedd by __getitem__\" of (src_indices, src_length, tgt_indices)\n",
    "    '''\n",
    "    src_batch, src_lengths, tgt_batch = zip(*batch)\n",
    "    # src_batch will be a list of source tensors, each of shape (max_src_len)\n",
    "    # src_lengths will be a list of source lengths(ignoring padding) \n",
    "    # tgt_batch will be a list of target tensors, each of shape (max_tgt_len)\n",
    "\n",
    "    # stacking these tensors to form a single tensor for each of the source and target sequences\n",
    "    src_batch = torch.stack(src_batch) # shape: (batch_size, max_src_len)\n",
    "    tgt_batch = torch.stack(tgt_batch) # shape: (batch_size, max_tgt_len)\n",
    "    src_lengths = torch.tensor(src_lengths) # shape: (batch_size)   \n",
    "    return src_batch, src_lengths, tgt_batch\n",
    "\n",
    "max_src_len = 26  \n",
    "max_tgt_len = 26  \n",
    "batch_size = 256\n",
    "train_dataset = TranslationDataset(src_sentences=english_tokenized, tgt_sentences=french_tokenized, src_vocab=english_vocab, tgt_vocab=french_vocab, max_src_len=max_src_len, max_tgt_len=max_tgt_len)\n",
    "# we used collate_fn to specify how to combine individual samples into batches; because here we are not simply stacking the tensors, but also need to keep track of the actual lengths of the sequences !!!!\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_batch(x),num_workers=4, pin_memory=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxvvVU3ezUHR"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEKujJUEzVux"
   },
   "source": [
    "After preprrocessing the data, we can build our model. Start by building a baseline architecture relying on one directional RNNs, LSTMs, or GRUs. It will be good to lookup how to build Seq2Seq models, there are some new layers that will help you like RepeatVector and TimeDistributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers=2):\n",
    "        '''\n",
    "        input_dim: The size of the input vocabulary (It tells the model how many unique tokens it can expect to see)\n",
    "\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) # it will have a matrix of size (input_dim, embedding_dim)\n",
    "        # where each row of the matrix will represent the embedding of a token in the vocabulary\n",
    "        '''Now each token in my input sequence is an index that refers to a row in this embedding matrix.'''\n",
    "        '''This lookup returns a new tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch size (batch_size, max_length, hidden_dim)\n",
    "        # note that in LSTM, the time step happens under the hood\n",
    "        # LSTM treats each position along sequence lenth as a time step\n",
    "        \n",
    "    def forward(self, src, src_lengths):\n",
    "        # src shape: (batch_size, max_length) - a batch of sequences of token indices\n",
    "        # src_lengths shape: (batch_size) - the actual lengths of each sequence without padding\n",
    "        '''If src has shape (batch_size, max_length) (a batch of padded sequences of token indices), the embedding layer’s output will be a tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        embedded = self.embedding(src)  # shape: (batch_size, max_length, embedding_dim)\n",
    "        '''LSTM will process only the non-padded elements'''\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False) # \n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden and cell shapes: (num_layers, batch_size, hidden_dim)\n",
    "        # Note if you need the outputs you need to unpack the sequences using pad_packed_sequence\n",
    "        # outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True) #outputs shape: (batch_size, max_length, hidden_dim)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers=2, teacher_forcing_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        # Embedding layer for the output vocabulary( French vocabulary)\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "    \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to generate predictions over vocabulary, which then by the use of the final softmax layer(which is applied by default in the cross-entropy loss, so we will NOT do it here) will give the probabilities of each token in the vocabulary\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token shape: (batch_size) - single token for each sequence in the batch, since later in the full model we will be looping over the sequence length(1 token at a time)\n",
    "        \n",
    "        '''Step 1: Embed the input token'''\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        '''Step 2: Pass the embedded input through the LSTM'''\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        '''Step 3: Generate prediction for the token'''\n",
    "        prediction = self.fc_out(output.squeeze(1))  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model (Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder,  teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_sequence_length = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "        # decoder_outputs (predictions at each time step)\n",
    "        outputs = torch.zeros(batch_size, tgt_sequence_length - 1, tgt_vocab_size)\n",
    "\n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # The first input to the decoder is the <SOS> token\n",
    "        input_token = tgt[:, 0]  # Shape: (batch_size)\n",
    "\n",
    "        for t in range(1, tgt_sequence_length):\n",
    "            # Pass the input token through the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t - 1, :] = output\n",
    "\n",
    "            # will i use teacher forcing?\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "\n",
    "            # Get the next input token\n",
    "            top1 = output.argmax(1) # maximum value along the second dimension\n",
    "            input_token = tgt[:, t] if teacher_force else top1\n",
    "        return outputs\n",
    "    def inference(self, src, src_lengths, max_len=26, start_token=1, end_token=2):\n",
    "        '''Will output the generated indices of the target sequence'''\n",
    "        # src: Source input sequence - Shape: (batch_size, src_sequence_length) (English)\n",
    "        # src_lengths: Actual lengths of each sequence in src (ignores padding)\n",
    "        # max_len: Maximum length of the generated sequence \n",
    "        # start_token: Start-of-sequence token index\n",
    "        # end_token: End-of-sequence token index\n",
    "        \n",
    "        '''Will output the generated indices of the target sequence'''\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # start the model with input token with the <SOS> token\n",
    "        input_token = torch.tensor([start_token] * batch_size)  # Shape: (batch_size)\n",
    "\n",
    "        # here we will store the generated tokens, and NOT the probabilities\n",
    "        generated_tokens = torch.zeros(batch_size, max_len).long()\n",
    "        for t in range(max_len):\n",
    "            # Pass the input token through the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "\n",
    "            # Get the predicted token\n",
    "            predicted_token = output.argmax(1)  # Shape: (batch_size)\n",
    "            generated_tokens[:, t] = predicted_token\n",
    "\n",
    "            \n",
    "            if (predicted_token == end_token).all(): # Check if all sequences have predicted <EOS>, then we are done generating\n",
    "                break\n",
    "            # Update input token for the next time step (like before)\n",
    "            input_token = predicted_token\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP10HtNBzpT0"
   },
   "source": [
    "Compile and train the model.\n",
    "**FYI:** While specifying the architecture of your model and the number of epochs for training, keeep in your mind that your model might take A LOT of time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(english_vocab)\n",
    "output_dim = len(french_vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "teacher_forcing_ratio = 0.5\n",
    "num_epochs = 52\n",
    "\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, embedding_dim, hidden_dim, num_layers, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model.load_state_dict(torch.load('seq2seq_model_52.pth',weights_only=True))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=french_vocab[\"<PAD>\"])\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(0,num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, src_lengths, tgt in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lengths, tgt)  # Shape: (batch_size, tgt_sequence_length - 1, output_dim)\n",
    "\n",
    "        # Reshape output and target for calculating loss !!!!\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)  # Exclude the first <SOS> token (because we don't need to calculate loss for it)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    # Calculate average loss for the epoch, plus saving the model, since this code is running on a remote server, so I need to save the model after each epoch, to compare these models later on!!!\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    s = f\"seq2seq_model_5{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), s)\n",
    "    print(f\"Epoch 5{epoch }/{num_epochs}, Loss: {avg_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UoEcxyJztiQ"
   },
   "source": [
    "Define a function that gets an input sentence in english and gives the output sentence in the french language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUU_RdCxYpM6"
   },
   "outputs": [],
   "source": [
    "def translate_to_french(sentence,  french_vocab_inverse = french_index_to_token ,english_vocab = english_vocab,max_src_len = 26):\n",
    "    '''\n",
    "    What this function will do is take an English Sentence, convert it to tokens, then convert to indicies, pass these indices to the model, use model.inference to generate the indices of the french sentence, the convert these indices to tokens and finally to a sentence\n",
    "    sentence: English sentence to translate\n",
    "    french_vocab_inverse: Inverse of the French vocabulary (index to token)\n",
    "    english_vocab: English vocabulary (token to index)\n",
    "    max_src_len: Maximum length of source sentences \n",
    "    '''\n",
    "    model.eval()\n",
    "    sentence = word_tokenize(sentence.lower())\n",
    "    indices = [english_vocab.get(token, english_vocab[\"<UNK>\"]) for token in sentence]\n",
    "    lengths = torch.tensor([min(len(indices), max_src_len)])\n",
    "\n",
    "    # Pad/Truncate \n",
    "    if len(indices) < max_src_len:\n",
    "        indices += [english_vocab[\"<PAD>\"]] * (max_src_len - len(indices))\n",
    "    elif len(indices) > max_src_len:\n",
    "        indices = indices[:max_src_len]\n",
    "    indices = torch.tensor(indices).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        translated_indices = model.inference(indices, lengths)\n",
    "\n",
    "    # indices to token (in fench vocab)\n",
    "    translated_tokens = []\n",
    "    for index in translated_indices[0]:\n",
    "        token = french_vocab_inverse[index.item()]\n",
    "        if token == \"<EOS>\":\n",
    "            break\n",
    "        translated_tokens.append(token)\n",
    "    return ' '.join(translated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUQIcAjWz3bt"
   },
   "source": [
    "Test the following sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDmNqnZIQMko"
   },
   "outputs": [],
   "source": [
    "input = \"she is driving the truck\"\n",
    "print(\"English:\", input)\n",
    "print(\"French:\", translate_to_french(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that in the report I included screen shots from a small website that I created to show case the result of the model (for the code I didn't provide it here since we were required to only submit the notepad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdI2XhaBz6CN"
   },
   "source": [
    "Try to improve your model by modifying the architecture to take into account bidirectionality which is very useful in Machine Translation. Create a new model called model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch28BLsbGnCn"
   },
   "outputs": [],
   "source": [
    "class Encoder_2(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers=2):\n",
    "        '''\n",
    "        input_dim: The size of the input vocabulary (It tells the model how many unique tokens it can expect to see)\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) # it will have a matrix of size (input_dim, embedding_dim)\n",
    "        # where each row of the matrix will represent the embedding of a token in the vocabulary\n",
    "        '''Now each token in my input sequence is an index that refers to a row in this embedding matrix.'''\n",
    "        '''This lookup returns a new tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        \n",
    "        '''Modification: Make the LSTM bidirectional by setting bidirectional=True'''\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True) # batch_first=True means that the first dimension of the input and output will be the batch size (batch_size, max_length, hidden_dim)\n",
    "        # note that in LSTM, the time step happens under the hood\n",
    "        # LSTM treats each position along sequence length as a time step\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # src shape: (batch_size, max_length) - a batch of sequences of token indices\n",
    "        # src_lengths shape: (batch_size) - the actual lengths of each sequence without padding\n",
    "\n",
    "        '''If src has shape (batch_size, max_length) (a batch of padded sequences of token indices), the embedding layer’s output will be a tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        embedded = self.embedding(src)  # shape: (batch_size, max_length, embedding_dim)\n",
    "        \n",
    "\n",
    "        '''LSTM will process only the non-padded elements'''\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False) \n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden and cell shapes: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        \n",
    "        '''Modification: Since the LSTM is bidirectional, we need to handle the hidden and cell states accordingly.\n",
    "        The hidden and cell states from the bidirectional LSTM have shapes:\n",
    "        - hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        We need to combine the hidden states from both directions to pass to the decoder.\n",
    "        One common approach is to sum the hidden states from both directions.\n",
    "        '''\n",
    "        # Reshape hidden and cell to (num_layers, num_directions, batch_size, hidden_dim)\n",
    "        num_layers = self.lstm.num_layers\n",
    "        num_directions = 2  # Because bidirectional=True\n",
    "        hidden = hidden.view(num_layers, num_directions, hidden.size(1), hidden.size(2))\n",
    "        cell = cell.view(num_layers, num_directions, cell.size(1), cell.size(2))\n",
    "\n",
    "        # Sum the hidden states from both directions\n",
    "        hidden = hidden.sum(dim=1)  # Now shape: (num_layers, batch_size, hidden_dim)\n",
    "        cell = cell.sum(dim=1)      # Now shape: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Note if you need the outputs you need to unpack the sequences using pad_packed_sequence\n",
    "        # outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True) #outputs shape: (batch_size, max_length, hidden_dim * num_directions)\n",
    "    \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder_2(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers=2, teacher_forcing_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        # Embedding layer for the output vocabulary( French vocabulary)\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "    \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to generate predictions over vocabulary\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token shape: (batch_size) - single token for each sequence in the batch, since later in the full model we will be looping over the sequence length(1 token at a time)\n",
    "        \n",
    "        '''Step 1: Embed the input token'''\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        '''Step 2: Pass the embedded input through the LSTM'''\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        '''Step 3: Generate prediction for the token'''\n",
    "        prediction = self.fc_out(output.squeeze(1))  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq_2(nn.Module):\n",
    "    def __init__(self, encoder, decoder,  teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_sequence_length = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        # Initialize tensor to hold decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_sequence_length - 1, tgt_vocab_size)\n",
    "\n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # The first input to the decoder is the <SOS> token\n",
    "        input_token = tgt[:, 0]  # Shape: (batch_size)\n",
    "\n",
    "        for t in range(1, tgt_sequence_length):\n",
    "            # Pass the input token through the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "\n",
    "            # Store the output prediction\n",
    "            outputs[:, t - 1, :] = output\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "\n",
    "            # Get the next input token\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input_token = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "    def inference(self, src, src_lengths, max_len=26, start_token=1, end_token=2):\n",
    "        '''Will output the generated indices of the target sequence'''\n",
    "        # src: Source input sequence - Shape: (batch_size, src_sequence_length) (English)\n",
    "        # src_lengths: Actual lengths of each sequence in src (ignores padding)\n",
    "        # max_len: Maximum length of the generated sequence \n",
    "        # start_token: Start-of-sequence token index\n",
    "        # end_token: End-of-sequence token index\n",
    "        \n",
    "        '''Will output the generated indices of the target sequence'''\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # Initialize the input token with the <SOS> token\n",
    "        input_token = torch.tensor([start_token] * batch_size)  # Shape: (batch_size)\n",
    "\n",
    "        # Initialize tensor to hold generated tokens\n",
    "        generated_tokens = torch.zeros(batch_size, max_len).long()\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # Pass the input token through the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "\n",
    "            # Get the predicted token\n",
    "            predicted_token = output.argmax(1)  # Shape: (batch_size)\n",
    "            generated_tokens[:, t] = predicted_token\n",
    "\n",
    "            # Check if all sequences have predicted <EOS>\n",
    "            if (predicted_token == end_token).all():\n",
    "                break\n",
    "\n",
    "            # Update input token for the next time step\n",
    "            input_token = predicted_token\n",
    "\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDvxt9L0C21"
   },
   "source": [
    "compile and train your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK1QvVmaTWI2"
   },
   "outputs": [],
   "source": [
    "max_src_len = 26  \n",
    "max_tgt_len = 26  \n",
    "batch_size = 32 \n",
    "train_dataset = TranslationDataset(src_sentences=english_tokenized, tgt_sentences=french_tokenized, src_vocab=english_vocab, tgt_vocab=french_vocab, max_src_len=max_src_len, max_tgt_len=max_tgt_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_batch(x))\n",
    "input_dim = len(english_vocab)\n",
    "output_dim = len(french_vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "teacher_forcing_ratio = 0 # here just for experimentation we pupt it 0 \n",
    "num_epochs = 15\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, embedding_dim, hidden_dim, num_layers, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model.load_state_dict(torch.load('seq2seq_model_9.pth',weights_only=True))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=french_vocab[\"<PAD>\"])\n",
    "print(\"Training the model...\")\n",
    "epoch_losses = []\n",
    "for epoch in range(9,num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, src_lengths, tgt in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the Seq2Seq model\n",
    "        output = model(src, src_lengths, tgt)  # Shape: (batch_size, tgt_sequence_length - 1, output_dim)\n",
    "\n",
    "        # Reshape output and target for calculating loss\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)  # Exclude the first <SOS> token\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "    s = f\"seq2seq_model_{epoch + 1}.pth\"\n",
    "    torch.save(model.state_dict(), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkpOI2JI0GBx"
   },
   "source": [
    "Define a new function that relies on your new model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gulu8OiXTbae"
   },
   "outputs": [],
   "source": [
    "def translate_to_french(sentence,  french_vocab_inverse = french_index_to_token ,english_vocab = english_vocab,max_src_len = 26):\n",
    "    '''\n",
    "    What this function will do is take an English Sentence, convert it to tokens, then convert to indicies, pass these indices to the model, use model.inference to generate the indices of the french sentence, the convert these indices to tokens and finally to a sentence\n",
    "    sentence: English sentence to translate\n",
    "    french_vocab_inverse: Inverse of the French vocabulary (index to token)\n",
    "    english_vocab: English vocabulary (token to index)\n",
    "    max_src_len: Maximum length of source sentences \n",
    "    '''\n",
    "    model.eval()\n",
    "    # Tokenize the sentence\n",
    "    sentence = word_tokenize(sentence.lower())\n",
    "    \n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        indices.append(english_vocab.get(token, english_vocab[\"<UNK>\"])) # if token not found, use the index of the <UNK> token\n",
    "    # Get the lengths of the sequences, before padding \n",
    "    lengths = torch.tensor([len(indices)]) # shape: (1)\n",
    "\n",
    "    # Pad the sequence\n",
    "    if len(indices) < max_src_len:\n",
    "        indices += [english_vocab[\"<PAD>\"]] * (max_src_len - len(indices))\n",
    "    # truncate \n",
    "    elif len(indices) > max_src_len:\n",
    "        indices = indices[:max_src_len]\n",
    "    indices = torch.tensor(indices).unsqueeze(0) # shape: (1, max_src_len)\n",
    "    with torch.no_grad():\n",
    "        translated_indices = model.inference(indices, lengths)\n",
    "    # index to token \n",
    "    translated_tokens = []\n",
    "    for index in translated_indices[0]:\n",
    "        token = french_vocab_inverse[index.item()]\n",
    "        if token == \"<EOS>\":\n",
    "            break\n",
    "        translated_tokens.append(token)\n",
    "    # to make it a string (sentence) not a list of tokens\n",
    "    return ' '.join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CO0pO6-UAeE"
   },
   "outputs": [],
   "source": [
    "input = \"she is driving the truck\"\n",
    "print(\"English:\", input)\n",
    "print(\"French:\", translate_to_french(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGeXrjqbZen7"
   },
   "source": [
    "**What is another adjustment in terms of architecture that you might be able to do to improve your model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bekjOkjbZlBf"
   },
   "source": [
    "Please see details in report, here are just brief answers ! \\\\\n",
    "One thing that we could do is add layers and maybe remove teacher's forcing to let the model independtly learn the patterns, and maybe adding dropout to prevent the model from associating words as always together (as discussed in the report and in the demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnIN2lD2tn05"
   },
   "source": [
    "**What are some additional ways that we can do to improve the performance of our model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7_MCCbQt3uq"
   },
   "source": [
    "Same here, I will be discussing just briefly. everything is included in the report and in the demo video. What we can do is also include attention mechanism to focus on differnt parts of the sentence while generating each token, to prevent problems that we also discuss in the report and in the demo :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bbEFdGGIYZX"
   },
   "source": [
    "# Video Recording Link\n",
    "\n",
    "**A short (10 minutes max) recorded video where you explain your solution.\n",
    "Make sure your face is visible in the video, as if you’re presenting your\n",
    "work during a job interview.**\n",
    "\n",
    "[Share The Link Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cA93K-TUbcR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
