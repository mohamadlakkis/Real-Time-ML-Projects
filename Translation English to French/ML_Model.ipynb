{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiC75uo6u_Of"
   },
   "source": [
    "##Machine Translation Using a Seq2Seq Architecture\n",
    "\n",
    "---\n",
    "The goal of this colab is to get you more familiar with the Seq2Seq models and their challenges. For this reason, you will be working on machine translation problem where we would have a sentence as input (in english), and the output is gonna be the translated sentence (in french). So just like what happens with Google Translate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeK4LPupvg_c"
   },
   "source": [
    "**Just to give you a heads up:** We won't be having a model performing like Google translate, but at least we will have an idea about how Google Translate works and the challenges that exist with a translation problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBTvDTzBv293"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_j1ZzS3v6N3"
   },
   "source": [
    "We start by importing numpy and pandas and then we can add the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0IARXAX1e1m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mnl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/mnl/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAcLqZ7uv-SJ"
   },
   "source": [
    "Upload your data here. Here is the [Drive link](https://drive.google.com/drive/folders/10ncj3w7kI9GPx_rz-WfKEGCv4Dz1EYf6?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3hLN42axOjn"
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0-M7cFxTPpqy"
   },
   "outputs": [],
   "source": [
    "en = pd.read_csv('Datasets/en.csv')\n",
    "fr = pd.read_csv('Datasets/fr.csv')\n",
    "english_sentences = en.iloc[:, 0]\n",
    "french_sentences = fr.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr8OO1OhwSp4"
   },
   "source": [
    "**How many sentences does each of the files contain?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XhWJP-b02HKq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137859, 137859)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences), len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITGJN5tIwkDO"
   },
   "source": [
    "Now let us concatenate the 2 dataframes into one dataframe that we call **df** where one column has the english senetnces and the other has the french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-ZXxahsB2njn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  the united states is usually chilly during jul...   \n",
      "1  california is usually quiet during march , and...   \n",
      "2  the united states is sometimes mild during jun...   \n",
      "3  your least liked fruit is the grape , but my l...   \n",
      "4  his favorite fruit is the orange , but my favo...   \n",
      "\n",
      "                                              French  \n",
      "0  les états-unis est généralement froid en juill...  \n",
      "1  california est généralement calme en mars , et...  \n",
      "2  les états-unis est parfois légère en juin , et...  \n",
      "3  votre moins aimé fruit est le raisin , mais mo...  \n",
      "4  son fruit préféré est l'orange , mais mon préf...  \n"
     ]
    }
   ],
   "source": [
    "en.columns = ['English']\n",
    "fr.columns = ['French']\n",
    "combined_df = pd.concat([en, fr], axis=1)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xc1TsEHw9yC"
   },
   "source": [
    "Pick a sentence and print it in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QuRVWch23ujo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.iloc[0, 0])\n",
    "print(combined_df.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQjXYP1txFCi"
   },
   "source": [
    "##Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgz6jIoVxHUF"
   },
   "source": [
    "The data that we have is almost clean as we can see, we just need to remove the punctuations inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6YYOt5QftcFI"
   },
   "outputs": [],
   "source": [
    "'''remove the punctuation'''\n",
    "combined_df['English'] = combined_df['English'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "combined_df['French'] = combined_df['French'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "''' convert to lowercase'''\n",
    "combined_df['English'] = combined_df['English'].apply(lambda x: x.lower())\n",
    "combined_df['French'] = combined_df['French'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C1qsC9LxZPb"
   },
   "source": [
    "Make sure that the punctuation is removed by printing the example that you printed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T80tiWxe84G7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the united states is usually chilly during july  and it is usually freezing in november ',\n",
       " 'les étatsunis est généralement froid en juillet  et il gèle habituellement en novembre ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[0, 0], combined_df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuFNjoBAx4oN"
   },
   "source": [
    "##Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATfefzPExi2k"
   },
   "source": [
    "Add a column **ENG Length** to the dataset that shows how many words does a sentence contain, and do the same for french in a column called **FR Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dakeo81s352S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  the united states is usually chilly during jul...   \n",
      "1  california is usually quiet during march  and ...   \n",
      "2  the united states is sometimes mild during jun...   \n",
      "3  your least liked fruit is the grape  but my le...   \n",
      "4  his favorite fruit is the orange  but my favor...   \n",
      "\n",
      "                                              French  ENG Length  FR Length  \n",
      "0  les étatsunis est généralement froid en juille...          15         13  \n",
      "1  california est généralement calme en mars  et ...          13         13  \n",
      "2  les étatsunis est parfois légère en juin  et i...          14         13  \n",
      "3  votre moins aimé fruit est le raisin  mais mon...          14         14  \n",
      "4  son fruit préféré est lorange  mais mon préfér...          12         11  \n"
     ]
    }
   ],
   "source": [
    "# add a column to the dataframe called \"ENG Length\" and \"FR Length\" that contains the length of the English and French sentences respectively\n",
    "combined_df['ENG Length'] = combined_df['English'].apply(lambda x: len(x.split()))\n",
    "combined_df['FR Length'] = combined_df['French'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjQLW0K5xwx1"
   },
   "source": [
    "Visualize the distribution of the lengths of english sentences and french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_q_UIMJ09L24"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(combined_df['ENG Length'], bins=10, alpha=0.5, label='English Sentence Lengths',density=True) # density=True normalizes the data\n",
    "plt.hist(combined_df['FR Length'], bins=10, alpha=0.5, label='French Sentence Lengths', density=True) # density=True normalizes the data (getting an approximation of the pdf)\n",
    "plt.xlabel('Sentence Length (Word Count)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths (English vs. French)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDXb2d9ix9DV"
   },
   "source": [
    "Get the maximum length of an english sentence and the maximum length of a french sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BpnBB04U_lHd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(15), np.int64(21))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_eng_length = combined_df['ENG Length'].max()\n",
    "max_fr_length = combined_df['FR Length'].max()\n",
    "max_eng_length, max_fr_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4s-spsRyGJv"
   },
   "source": [
    "##Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0ZmIT2GyJMU"
   },
   "source": [
    "In order for the data to be fed to the model, it has to be tokenized and padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0r9z-eErm9H"
   },
   "source": [
    "####Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5L_zkhfyQuX"
   },
   "source": [
    "**To tokenize english and french sentences, we can use only one tokenizer. True or False?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z0ZcNOeyauD"
   },
   "source": [
    "Wrong, in this case I am using basic word tokenization through nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "814mKDFiymcY"
   },
   "source": [
    "Tokenize the sentences that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aiXlciqFuQzW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'united', 'states', 'is', 'usually', 'chilly', 'during', 'july', 'and', 'it', 'is', 'usually', 'freezing', 'in', 'november']\n",
      "['les', 'étatsunis', 'est', 'généralement', 'froid', 'en', 'juillet', 'et', 'il', 'gèle', 'habituellement', 'en', 'novembre']\n",
      "137859 137859\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    return [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "english_sentences = combined_df['English']\n",
    "french_sentences = combined_df['French']\n",
    "english_tokenized = tokenize_sentences(english_sentences)\n",
    "french_tokenized = tokenize_sentences(french_sentences)\n",
    "print(english_tokenized[0])\n",
    "print(french_tokenized[0])\n",
    "print(len(english_tokenized), len(french_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUN01jDXys9B"
   },
   "source": [
    "**How many unique words do we have in english and in french?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1WahkdzKvIlO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 203\n",
      "French Vocabulary Size: 348\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'the': 4, 'united': 5, 'states': 6, 'is': 7, 'usually': 8, 'chilly': 9, 'during': 10, 'july': 11, 'and': 12, 'it': 13, 'freezing': 14, 'in': 15, 'november': 16, 'california': 17, 'quiet': 18, 'march': 19, 'hot': 20, 'june': 21, 'sometimes': 22, 'mild': 23, 'cold': 24, 'september': 25, 'your': 26, 'least': 27, 'liked': 28, 'fruit': 29, 'grape': 30, 'but': 31, 'my': 32, 'apple': 33, 'his': 34, 'favorite': 35, 'orange': 36, 'paris': 37, 'relaxing': 38, 'december': 39, 'new': 40, 'jersey': 41, 'busy': 42, 'spring': 43, 'never': 44, 'our': 45, 'lemon': 46, 'january': 47, 'warm': 48, 'lime': 49, 'her': 50, 'banana': 51, 'he': 52, 'saw': 53, 'a': 54, 'old': 55, 'yellow': 56, 'truck': 57, 'india': 58, 'rainy': 59, 'that': 60, 'cat': 61, 'was': 62, 'most': 63, 'loved': 64, 'animal': 65, 'dislikes': 66, 'grapefruit': 67, 'limes': 68, 'lemons': 69, 'february': 70, 'china': 71, 'pleasant': 72, 'autumn': 73, 'october': 74, 'wonderful': 75, 'nice': 76, 'april': 77, 'summer': 78, 'france': 79, 'snowy': 80, 'may': 81, 'grapes': 82, 'mangoes': 83, 'their': 84, 'mango': 85, 'pear': 86, 'august': 87, 'beautiful': 88, 'apples': 89, 'peaches': 90, 'feared': 91, 'shark': 92, 'wet': 93, 'dry': 94, 'we': 95, 'like': 96, 'oranges': 97, 'they': 98, 'pears': 99, 'she': 100, 'little': 101, 'red': 102, 'winter': 103, 'disliked': 104, 'rusty': 105, 'car': 106, 'strawberries': 107, 'i': 108, 'strawberry': 109, 'bananas': 110, 'going': 111, 'to': 112, 'next': 113, 'plan': 114, 'visit': 115, 'elephants': 116, 'were': 117, 'animals': 118, 'are': 119, 'likes': 120, 'dislike': 121, 'fall': 122, 'driving': 123, 'peach': 124, 'drives': 125, 'blue': 126, 'you': 127, 'bird': 128, 'horses': 129, 'mouse': 130, 'went': 131, 'last': 132, 'horse': 133, 'automobile': 134, 'dogs': 135, 'white': 136, 'elephant': 137, 'black': 138, 'think': 139, 'difficult': 140, 'translate': 141, 'between': 142, 'spanish': 143, 'portuguese': 144, 'big': 145, 'green': 146, 'translating': 147, 'fun': 148, 'where': 149, 'dog': 150, 'why': 151, 'might': 152, 'go': 153, 'this': 154, 'drove': 155, 'shiny': 156, 'sharks': 157, 'monkey': 158, 'how': 159, 'weather': 160, 'lion': 161, 'plans': 162, 'bear': 163, 'rabbit': 164, 'its': 165, 'chinese': 166, 'when': 167, 'eiffel': 168, 'tower': 169, 'did': 170, 'grocery': 171, 'store': 172, 'wanted': 173, 'does': 174, 'football': 175, 'field': 176, 'wants': 177, 'didnt': 178, 'snake': 179, 'snakes': 180, 'do': 181, 'easy': 182, 'thinks': 183, 'english': 184, 'french': 185, 'would': 186, 'arent': 187, 'cats': 188, 'rabbits': 189, 'has': 190, 'been': 191, 'monkeys': 192, 'lake': 193, 'bears': 194, 'school': 195, 'birds': 196, 'want': 197, 'isnt': 198, 'lions': 199, 'am': 200, 'mice': 201, 'have': 202}\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(tokenized_sentences, special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]):\n",
    "    all_tokens_list = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        all_tokens_list.extend(sentence)\n",
    "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    for token in all_tokens_list:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab) # updating each token with a unique index, since len(vocab) is changing each time\n",
    "    # Decoding Later \n",
    "    index_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    return vocab, index_to_token\n",
    "english_vocab, english_index_to_token = build_vocabulary(english_tokenized)\n",
    "french_vocab, french_index_to_token = build_vocabulary(french_tokenized)\n",
    "\n",
    "print(\"English Vocabulary Size:\", len(english_vocab))\n",
    "print(\"French Vocabulary Size:\", len(french_vocab))\n",
    "print(english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0C2RJjArtJd"
   },
   "source": [
    "####Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXdTXMo5y8oB"
   },
   "source": [
    "**What should be the length of the sequences that we have after padding?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wtHQsgXzImq"
   },
   "source": [
    "As the padding length, in this case I will take as the length of the maximum phrase here!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRXayRzVzQD4"
   },
   "source": [
    "Perform padding on the sequences that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oNdO9EZrxvmN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English Sequence: [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 7, 8, 14, 15, 16, 2, 0, 0, 0]\n",
      "Sample French Sequence: [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sequence(tokenized_sentences, vocab, max_length):\n",
    "    '''\n",
    "    params: tokenized_sentences: List of sentences, each sentence is a list of tokens\n",
    "            vocab: Dictionary containing the vocabulary where the keys are the tokens and the values are the indices\n",
    "            max_length: Maximum length of the sequence\n",
    "    return: List of sequences where each sequence is a list of indices representing the tokens in the input sentences\n",
    "    '''\n",
    "    sequences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sequence = [vocab[\"<SOS>\"]]\n",
    "\n",
    "        # Convert each token to its index in the vocabulary\n",
    "        sequence += [vocab.get(token, vocab[\"<UNK>\"]) for token in sentence]\n",
    "\n",
    "        # Add <EOS> at the end of each sequence\n",
    "        sequence.append(vocab[\"<EOS>\"])\n",
    "        if len(sequence) > max_length:\n",
    "\n",
    "            sequence = sequence[:max_length] # truncate the sequence to the maximum length (this will be used for new phrases( which won't necessarlily have a length of at most max_length))\n",
    "\n",
    "        sequence += [vocab[\"<PAD>\"]] * (max_length - len(sequence)) # fill the rest with <PAD> tokens (which has index 0)\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "# Note here I chose to pad to the right\n",
    "english_sequences = convert_to_sequence(english_tokenized, english_vocab,max_eng_length+5)\n",
    "french_sequences = convert_to_sequence(french_tokenized, french_vocab, max_fr_length+5) # 1 to inclde <EOS> token\n",
    "print(\"Sample English Sequence:\", english_sequences[0])\n",
    "print(\"Sample French Sequence:\", french_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Batch Shape: torch.Size([32, 20])\n",
      "French Batch Shape: torch.Size([32, 26])\n"
     ]
    }
   ],
   "source": [
    "english_tensor = torch.tensor(english_sequences, dtype=torch.long)\n",
    "french_tensor = torch.tensor(french_sequences, dtype=torch.long)\n",
    "\n",
    "# Create a dataset and data loader for batching\n",
    "dataset = TensorDataset(english_tensor, french_tensor)\n",
    "batch_size = 32  # Example batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for english_batch, french_batch in dataloader:\n",
    "    print(\"English Batch Shape:\", english_batch.shape) # batch_size x max_length\n",
    "    print(\"French Batch Shape:\", french_batch.shape) # batch_size x max_length\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxvvVU3ezUHR"
   },
   "source": [
    "##Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEKujJUEzVux"
   },
   "source": [
    "After preprrocessing the data, we can build our model. Start by building a baseline architecture relying on one directional RNNs, LSTMs, or GRUs. It will be good to lookup how to build Seq2Seq models, there are some new layers that will help you like RepeatVector and TimeDistributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State Shape: torch.Size([2, 10, 512])\n",
      "Cell State Shape: torch.Size([2, 10, 512])\n",
      "Encoder output verification passed!\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers=2):\n",
    "        '''\n",
    "        input_dim: The size of the input vocabulary (It tells the model how many unique tokens it can expect to see)\n",
    "\n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) # it will have a matrix of size (input_dim, embedding_dim)\n",
    "        # where each row of the matrix will represent the embedding of a token in the vocabulary\n",
    "        '''Now each token in my input sequence is an index that refers to a row in this embedding matrix.'''\n",
    "        '''This lookup returns a new tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch size (batch_size, max_length, hidden_dim)\n",
    "        # note that in LSTM, the time step happens under the hood\n",
    "        # LSTM treats each position along sequence lenth as a time step\n",
    "        \n",
    "    def forward(self, src, src_lengths):\n",
    "        # src shape: (batch_size, max_length) - a batch of sequences of token indices\n",
    "        # src_lengths shape: (batch_size) - the actual lengths of each sequence without padding\n",
    "\n",
    "\n",
    "        '''If src has shape (batch_size, max_length) (a batch of padded sequences of token indices), the embedding layer’s output will be a tensor of shape (batch_size, max_length, embedding_dim)'''\n",
    "        embedded = self.embedding(src)  # shape: (batch_size, max_length, embedding_dim)\n",
    "        \n",
    "\n",
    "        '''LSTM will process only the non-padded elements'''\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False) # \n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden and cell shapes: (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # Note if you need the outputs you need to unpack the sequences using pad_packed_sequence\n",
    "        # outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True) #outputs shape: (batch_size, max_length, hidden_dim)\n",
    "    \n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "'''Let's test the encoder with a batch of 10 sequences'''\n",
    "actual_lengths = torch.tensor([len([token for token in sentence if token != english_vocab[\"<PAD>\"]]) for sentence in english_tensor[:10]])\n",
    "\n",
    "input_dim = len(english_vocab)  # Vocabulary size\n",
    "embedding_dim = 256             # Embedding dimension\n",
    "hidden_dim = 512                # Hidden dimension\n",
    "num_layers = 2                  # Number of layers in the LSTM\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, num_layers)\n",
    "# forward pass \n",
    "hidden, cell = encoder(english_tensor, actual_lengths)\n",
    "print(\"Hidden State Shape:\", hidden.shape)  # Expected: (num_layers, batch_size, hidden_dim)\n",
    "print(\"Cell State Shape:\", cell.shape)      # Expected: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "assert hidden.shape == (num_layers, english_tensor[:10].size(0), hidden_dim), \"Hidden state shape mismatch!\"\n",
    "assert cell.shape == (num_layers, english_tensor[:10].size(0), hidden_dim), \"Cell state shape mismatch!\"\n",
    "\n",
    "print(\"Encoder output verification passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers=2, teacher_forcing_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        # Embedding layer for the output vocabulary( French vocabulary)\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "    \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to generate predictions over vocabulary\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token shape: (batch_size) - single token for each sequence in the batch, since later in the full model we will be looping over the sequence length(1 token at a time)\n",
    "        \n",
    "        '''Step 1: Embed the input token'''\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # Shape: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        '''Step 2: Pass the embedded input through the LSTM'''\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        '''Step 3: Generate prediction for the next token'''\n",
    "        prediction = self.fc_out(output.squeeze(1))  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model (Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder,  teacher_forcing_ratio=0.5):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    def forward(self, src, src_lengths, tgt):\n",
    "        # src: Source input sequence (English) - Shape: (batch_size, src_sequence_length)\n",
    "        # src_lengths: Actual lengths of each sequence in src (ignores padding)\n",
    "        # tgt: Target sequence (French) - Shape: (batch_size, tgt_sequence_length)\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        tgt_sequence_length = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        '''Decoder outputs tensor'''\n",
    "        outputs = torch.zeros(batch_size, tgt_sequence_length, tgt_vocab_size) # output the full prob. distribution over the output vocabulary for each time step in the target sequence, for thecomputing the loss function, but in inference we will only use the argmax of this distribution\n",
    "\n",
    "        '''Get the hidden and cell states from the encoder'''\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        '''The first input to the decoder is the <SOS> token for each sequence'''\n",
    "        input_token = tgt[:, 0]  # Shape: (batch_size)\n",
    "\n",
    "        '''Go over each token in the target sequence'''\n",
    "        for t in range(1, tgt_sequence_length):\n",
    "            '''Input token: is the current token that we’re feeding to the decoder'''\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            \n",
    "            # Store the output prediction for this time step\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            '''Determine if we should use teacher forcing'''\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "    def inference(self, src, src_lengths, max_len=23, start_token=1, end_token=2):\n",
    "        # src: Source input sequence - Shape: (batch_size, src_sequence_length) (English)\n",
    "        # src_lengths: Actual lengths of each sequence in src (ignores padding)\n",
    "        # max_len: Maximum length of the generated sequence \n",
    "        # start_token: Start-of-sequence token index\n",
    "        # end_token: End-of-sequence token index\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        '''Encode the source sequence'''\n",
    "        hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        '''Initialize the input token with the <SOS> token, for each sequence in the batch'''\n",
    "        input_token = torch.tensor([start_token] * batch_size) # Shape: (batch_size)\n",
    "\n",
    "        generated_tokens = torch.zeros(batch_size, max_len).long() # storing the token indices\n",
    "\n",
    "        for t in range(max_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            \n",
    "            '''Get the most likely token (predicted token) at this time step'''\n",
    "            input_token = output.argmax(1)  # Shape: (batch_size)\n",
    "            generated_tokens[:, t] = input_token\n",
    "            \n",
    "            '''Stop generating if all sequences in the batch reach <EOS>'''\n",
    "            if (input_token == end_token).all():\n",
    "                break\n",
    "        return generated_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP10HtNBzpT0"
   },
   "source": [
    "Compile and train the model.\n",
    "**FYI:** While specifying the architecture of your model and the number of epochs for training, keeep in your mind that your model might take A LOT of time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data taking care f the train_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_src_len, max_tgt_len):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get source and target sentences\n",
    "        src_sentence = self.src_sentences[idx]\n",
    "        tgt_sentence = self.tgt_sentences[idx]\n",
    "        \n",
    "        # Convert tokens to indices and pad to max length\n",
    "        src_indices = self._convert_and_pad(src_sentence, self.src_vocab, self.max_src_len, \"<PAD>\")\n",
    "        tgt_indices = self._convert_and_pad(tgt_sentence, self.tgt_vocab, self.max_tgt_len, \"<PAD>\")\n",
    "\n",
    "        # Get actual lengths (ignoring <PAD> tokens)\n",
    "        src_length = min(len(src_sentence), self.max_src_len)\n",
    "        tgt_length = min(len(tgt_sentence), self.max_tgt_len)\n",
    "\n",
    "        return torch.tensor(src_indices), src_length, torch.tensor(tgt_indices)\n",
    "    \n",
    "    def _convert_and_pad(self, sentence, vocab, max_len, pad_token):\n",
    "        # Convert tokens to indices\n",
    "        indices = [vocab.get(token, vocab[\"<UNK>\"]) for token in sentence]\n",
    "        # Truncate if necessary and pad to max length\n",
    "        indices = indices[:max_len] + [vocab[pad_token]] * (max_len - len(indices))\n",
    "        return indices\n",
    "\n",
    "\n",
    "max_src_len = 25  \n",
    "max_tgt_len = 25  \n",
    "batch_size = 32   # Batch size\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TranslationDataset(\n",
    "    src_sentences=english_tokenized, \n",
    "    tgt_sentences=french_tokenized, \n",
    "    src_vocab=english_vocab, \n",
    "    tgt_vocab=french_vocab, \n",
    "    max_src_len=max_src_len, \n",
    "    max_tgt_len=max_tgt_len\n",
    ")\n",
    "def collate_batch(batch, max_src_len, max_tgt_len):\n",
    "    src_batch, src_lengths, tgt_batch = zip(*batch)\n",
    "    \n",
    "    # Stack each element of the batch to create the batch tensors\n",
    "    src_batch = torch.stack(src_batch)\n",
    "    tgt_batch = torch.stack(tgt_batch)\n",
    "    src_lengths = torch.tensor(src_lengths)\n",
    "\n",
    "    return src_batch, src_lengths, tgt_batch\n",
    "\n",
    "# Initialize the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_batch(x, max_src_len, max_tgt_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lWw4nBNIFp9D"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = len(english_vocab)\n",
    "output_dim = len(french_vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "teacher_forcing_ratio = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the encoder, decoder, and Seq2Seq model\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, embedding_dim, hidden_dim, num_layers, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=french_vocab[\"<PAD>\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_lengths, tgt in train_dataloader:  # Assume train_dataloader yields batches of data\n",
    "        src, src_lengths, tgt = src, src_lengths, tgt\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the Seq2Seq model\n",
    "        output = model(src, src_lengths, tgt)  # Shape: (batch_size, tgt_sequence_length, output_dim)\n",
    "        \n",
    "        # Reshape output and target for calculating loss\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  # Remove the first timestep (<SOS>)\n",
    "        tgt = tgt[:, 1:].reshape(-1)  # Align target sequence to ignore <SOS>\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UoEcxyJztiQ"
   },
   "source": [
    "Define a function that gets an input sentence in english and gives the output sentence in the french language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUU_RdCxYpM6"
   },
   "outputs": [],
   "source": [
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUQIcAjWz3bt"
   },
   "source": [
    "Test the following sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDmNqnZIQMko"
   },
   "outputs": [],
   "source": [
    "input = \"she is driving the truck\"\n",
    "\n",
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdI2XhaBz6CN"
   },
   "source": [
    "Try to improve your model by modifying the architecture to take into account bidirectionality which is very useful in Machine Translation. Create a new model called model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch28BLsbGnCn"
   },
   "outputs": [],
   "source": [
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDvxt9L0C21"
   },
   "source": [
    "compile and train your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK1QvVmaTWI2"
   },
   "outputs": [],
   "source": [
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkpOI2JI0GBx"
   },
   "source": [
    "Define a new function that relies on your new model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gulu8OiXTbae"
   },
   "outputs": [],
   "source": [
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CO0pO6-UAeE"
   },
   "outputs": [],
   "source": [
    "input = \"she is driving the truck\"\n",
    "\n",
    "#Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGeXrjqbZen7"
   },
   "source": [
    "**What is another adjustment in terms of architecture that you might be able to do to improve your model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bekjOkjbZlBf"
   },
   "source": [
    "[Share Your Knowledge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnIN2lD2tn05"
   },
   "source": [
    "**What are some additional ways that we can do to improve the performance of our model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7_MCCbQt3uq"
   },
   "source": [
    "[Share Your Knowledge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bbEFdGGIYZX"
   },
   "source": [
    "# Video Recording Link\n",
    "\n",
    "**A short (10 minutes max) recorded video where you explain your solution.\n",
    "Make sure your face is visible in the video, as if you’re presenting your\n",
    "work during a job interview.**\n",
    "\n",
    "[Share The Link Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cA93K-TUbcR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
